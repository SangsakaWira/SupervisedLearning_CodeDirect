{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARY\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense\n",
    "from keras import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4893</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4894</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4895</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4896</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4897</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.0              0.27         0.36            20.7      0.045   \n",
       "1               6.3              0.30         0.34             1.6      0.049   \n",
       "2               8.1              0.28         0.40             6.9      0.050   \n",
       "3               7.2              0.23         0.32             8.5      0.058   \n",
       "4               7.2              0.23         0.32             8.5      0.058   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4893            6.2              0.21         0.29             1.6      0.039   \n",
       "4894            6.6              0.32         0.36             8.0      0.047   \n",
       "4895            6.5              0.24         0.19             1.2      0.041   \n",
       "4896            5.5              0.29         0.30             1.1      0.022   \n",
       "4897            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    45.0                 170.0  1.00100  3.00       0.45   \n",
       "1                    14.0                 132.0  0.99400  3.30       0.49   \n",
       "2                    30.0                  97.0  0.99510  3.26       0.44   \n",
       "3                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "4                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "4893                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "4894                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "4895                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "4896                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "4897                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         8.8        6  \n",
       "1         9.5        6  \n",
       "2        10.1        6  \n",
       "3         9.9        6  \n",
       "4         9.9        6  \n",
       "...       ...      ...  \n",
       "4893     11.2        6  \n",
       "4894      9.6        5  \n",
       "4895      9.4        6  \n",
       "4896     12.8        7  \n",
       "4897     11.8        6  \n",
       "\n",
       "[4898 rows x 12 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT DATASET\n",
    "dataset = pd.read_csv(\"Dataset/winequality-white.csv\",sep=\";\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INISIASI INPUT & OUTPUT\n",
    "x = dataset.iloc[:,:-1]\n",
    "y = pd.get_dummies(dataset[\"quality\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4893</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4894</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4895</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4896</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4897</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      3  4  5  6  7  8  9\n",
       "0     0  0  0  1  0  0  0\n",
       "1     0  0  0  1  0  0  0\n",
       "2     0  0  0  1  0  0  0\n",
       "3     0  0  0  1  0  0  0\n",
       "4     0  0  0  1  0  0  0\n",
       "...  .. .. .. .. .. .. ..\n",
       "4893  0  0  0  1  0  0  0\n",
       "4894  0  0  1  0  0  0  0\n",
       "4895  0  0  0  1  0  0  0\n",
       "4896  0  0  0  0  1  0  0\n",
       "4897  0  0  0  1  0  0  0\n",
       "\n",
       "[4898 rows x 7 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.values\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.  ,  0.27,  0.36, ...,  3.  ,  0.45,  8.8 ],\n",
       "       [ 6.3 ,  0.3 ,  0.34, ...,  3.3 ,  0.49,  9.5 ],\n",
       "       [ 8.1 ,  0.28,  0.4 , ...,  3.26,  0.44, 10.1 ],\n",
       "       ...,\n",
       "       [ 6.5 ,  0.24,  0.19, ...,  2.99,  0.46,  9.4 ],\n",
       "       [ 5.5 ,  0.29,  0.3 , ...,  3.34,  0.38, 12.8 ],\n",
       "       [ 6.  ,  0.21,  0.38, ...,  3.26,  0.32, 11.8 ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = keras.Sequential([\n",
    "keras.layers.Dense(179, activation=tf.nn.relu, input_shape=[11]),\n",
    "keras.layers.Dense(138, activation=tf.nn.relu),\n",
    "keras.layers.Dense(108, activation=tf.nn.relu),\n",
    "keras.layers.Dense(88, activation=tf.nn.relu),\n",
    "keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "keras.layers.Dense(7, activation=tf.nn.relu)\n",
    "])\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4163 samples, validate on 735 samples\n",
      "Epoch 1/100\n",
      "4163/4163 [==============================] - 1s 299us/step - loss: 4.1120 - acc: 0.4329 - val_loss: 2.1037 - val_acc: 0.4585\n",
      "Epoch 2/100\n",
      "4163/4163 [==============================] - 1s 145us/step - loss: 2.1624 - acc: 0.4470 - val_loss: 2.0962 - val_acc: 0.4585\n",
      "Epoch 3/100\n",
      "4163/4163 [==============================] - 1s 145us/step - loss: 2.1502 - acc: 0.4518 - val_loss: 2.0736 - val_acc: 0.4612\n",
      "Epoch 4/100\n",
      "4163/4163 [==============================] - 1s 145us/step - loss: 2.1309 - acc: 0.4480 - val_loss: 2.0490 - val_acc: 0.4694\n",
      "Epoch 5/100\n",
      "4163/4163 [==============================] - 1s 147us/step - loss: 2.1377 - acc: 0.4504 - val_loss: 2.0507 - val_acc: 0.4735\n",
      "Epoch 6/100\n",
      "4163/4163 [==============================] - 1s 153us/step - loss: 2.1071 - acc: 0.4581 - val_loss: 2.0231 - val_acc: 0.4912\n",
      "Epoch 7/100\n",
      "4163/4163 [==============================] - 1s 152us/step - loss: 2.1031 - acc: 0.4677 - val_loss: 2.0280 - val_acc: 0.4871\n",
      "Epoch 8/100\n",
      "4163/4163 [==============================] - 1s 145us/step - loss: 2.1057 - acc: 0.4514 - val_loss: 2.0642 - val_acc: 0.4599\n",
      "Epoch 9/100\n",
      "4163/4163 [==============================] - 1s 147us/step - loss: 2.0896 - acc: 0.4691 - val_loss: 2.0224 - val_acc: 0.4789\n",
      "Epoch 10/100\n",
      "4163/4163 [==============================] - 1s 148us/step - loss: 2.0874 - acc: 0.4763 - val_loss: 2.0693 - val_acc: 0.4286\n",
      "Epoch 11/100\n",
      "4163/4163 [==============================] - 1s 148us/step - loss: 2.0843 - acc: 0.4768 - val_loss: 2.0190 - val_acc: 0.4776\n",
      "Epoch 12/100\n",
      "4163/4163 [==============================] - 1s 147us/step - loss: 2.0836 - acc: 0.4619 - val_loss: 1.9951 - val_acc: 0.4816\n",
      "Epoch 13/100\n",
      "4163/4163 [==============================] - 1s 151us/step - loss: 2.0790 - acc: 0.4737 - val_loss: 2.0188 - val_acc: 0.4599\n",
      "Epoch 14/100\n",
      "4163/4163 [==============================] - 1s 162us/step - loss: 2.0743 - acc: 0.4807 - val_loss: 1.9974 - val_acc: 0.5102\n",
      "Epoch 15/100\n",
      "4163/4163 [==============================] - 1s 153us/step - loss: 2.0760 - acc: 0.4771 - val_loss: 1.9826 - val_acc: 0.4912\n",
      "Epoch 16/100\n",
      "4163/4163 [==============================] - 1s 148us/step - loss: 2.0712 - acc: 0.4819 - val_loss: 2.0243 - val_acc: 0.4531\n",
      "Epoch 17/100\n",
      "4163/4163 [==============================] - 1s 151us/step - loss: 2.0642 - acc: 0.4792 - val_loss: 1.9939 - val_acc: 0.4680\n",
      "Epoch 18/100\n",
      "4163/4163 [==============================] - 1s 159us/step - loss: 2.0632 - acc: 0.4859 - val_loss: 2.0690 - val_acc: 0.4245\n",
      "Epoch 19/100\n",
      "4163/4163 [==============================] - 1s 156us/step - loss: 2.0649 - acc: 0.4816 - val_loss: 1.9847 - val_acc: 0.5048\n",
      "Epoch 20/100\n",
      "4163/4163 [==============================] - 1s 146us/step - loss: 2.0650 - acc: 0.4869 - val_loss: 2.4924 - val_acc: 0.4517\n",
      "Epoch 21/100\n",
      "4163/4163 [==============================] - 1s 149us/step - loss: 2.2012 - acc: 0.4463 - val_loss: 2.0788 - val_acc: 0.4585\n",
      "Epoch 22/100\n",
      "4163/4163 [==============================] - 1s 150us/step - loss: 2.0969 - acc: 0.4614 - val_loss: 1.9947 - val_acc: 0.4844\n",
      "Epoch 23/100\n",
      "4163/4163 [==============================] - 1s 164us/step - loss: 2.0660 - acc: 0.4850 - val_loss: 2.0258 - val_acc: 0.4612\n",
      "Epoch 24/100\n",
      "4163/4163 [==============================] - 1s 154us/step - loss: 2.0604 - acc: 0.4778 - val_loss: 1.9909 - val_acc: 0.4884\n",
      "Epoch 25/100\n",
      "4163/4163 [==============================] - 1s 151us/step - loss: 2.0575 - acc: 0.4821 - val_loss: 1.9843 - val_acc: 0.4871\n",
      "Epoch 26/100\n",
      "4163/4163 [==============================] - 1s 163us/step - loss: 2.0506 - acc: 0.4948 - val_loss: 2.0008 - val_acc: 0.5088\n",
      "Epoch 27/100\n",
      "4163/4163 [==============================] - 1s 151us/step - loss: 2.0490 - acc: 0.4883 - val_loss: 1.9603 - val_acc: 0.5129\n",
      "Epoch 28/100\n",
      "4163/4163 [==============================] - 1s 147us/step - loss: 2.0803 - acc: 0.4804 - val_loss: 2.0011 - val_acc: 0.4707\n",
      "Epoch 29/100\n",
      "4163/4163 [==============================] - 1s 148us/step - loss: 2.0617 - acc: 0.4927 - val_loss: 1.9732 - val_acc: 0.5224\n",
      "Epoch 30/100\n",
      "4163/4163 [==============================] - 1s 149us/step - loss: 2.0503 - acc: 0.4996 - val_loss: 1.9692 - val_acc: 0.5129\n",
      "Epoch 31/100\n",
      "4163/4163 [==============================] - 1s 151us/step - loss: 2.0430 - acc: 0.4972 - val_loss: 2.0118 - val_acc: 0.4517\n",
      "Epoch 32/100\n",
      "4163/4163 [==============================] - 1s 153us/step - loss: 2.0458 - acc: 0.4972 - val_loss: 1.9648 - val_acc: 0.5306\n",
      "Epoch 33/100\n",
      "4163/4163 [==============================] - 1s 159us/step - loss: 1.8530 - acc: 0.4965 - val_loss: 1.7637 - val_acc: 0.5007\n",
      "Epoch 34/100\n",
      "4163/4163 [==============================] - 1s 151us/step - loss: 1.6924 - acc: 0.4893 - val_loss: 1.7896 - val_acc: 0.4844\n",
      "Epoch 35/100\n",
      "4163/4163 [==============================] - 1s 149us/step - loss: 1.6791 - acc: 0.4896 - val_loss: 1.8088 - val_acc: 0.4816\n",
      "Epoch 36/100\n",
      "4163/4163 [==============================] - 1s 167us/step - loss: 1.6485 - acc: 0.4888 - val_loss: 1.6712 - val_acc: 0.5252\n",
      "Epoch 37/100\n",
      "4163/4163 [==============================] - 1s 149us/step - loss: 1.6300 - acc: 0.4900 - val_loss: 1.7586 - val_acc: 0.5061\n",
      "Epoch 38/100\n",
      "4163/4163 [==============================] - 1s 165us/step - loss: 1.6525 - acc: 0.4944 - val_loss: 1.7575 - val_acc: 0.4789\n",
      "Epoch 39/100\n",
      "4163/4163 [==============================] - 1s 158us/step - loss: 1.6197 - acc: 0.4994 - val_loss: 1.6650 - val_acc: 0.5075\n",
      "Epoch 40/100\n",
      "4163/4163 [==============================] - 1s 151us/step - loss: 1.6384 - acc: 0.4953 - val_loss: 1.6767 - val_acc: 0.5075\n",
      "Epoch 41/100\n",
      "4163/4163 [==============================] - 1s 148us/step - loss: 1.6271 - acc: 0.5001 - val_loss: 1.7938 - val_acc: 0.4925\n",
      "Epoch 42/100\n",
      "4163/4163 [==============================] - 1s 153us/step - loss: 1.7954 - acc: 0.4247 - val_loss: 1.8917 - val_acc: 0.4585\n",
      "Epoch 43/100\n",
      "4163/4163 [==============================] - 1s 149us/step - loss: 1.7727 - acc: 0.4470 - val_loss: 1.8088 - val_acc: 0.4585\n",
      "Epoch 44/100\n",
      "4163/4163 [==============================] - 1s 162us/step - loss: 1.7250 - acc: 0.4478 - val_loss: 1.7800 - val_acc: 0.4626\n",
      "Epoch 45/100\n",
      "4163/4163 [==============================] - 1s 152us/step - loss: 1.7062 - acc: 0.4506 - val_loss: 1.7349 - val_acc: 0.4680\n",
      "Epoch 46/100\n",
      "4163/4163 [==============================] - 1s 155us/step - loss: 1.6754 - acc: 0.4566 - val_loss: 1.7140 - val_acc: 0.4776\n",
      "Epoch 47/100\n",
      "4163/4163 [==============================] - 1s 152us/step - loss: 1.6438 - acc: 0.4747 - val_loss: 1.6954 - val_acc: 0.4939\n",
      "Epoch 48/100\n",
      "4163/4163 [==============================] - 1s 150us/step - loss: 1.6458 - acc: 0.4879 - val_loss: 1.6900 - val_acc: 0.5116\n",
      "Epoch 49/100\n",
      "4163/4163 [==============================] - 1s 153us/step - loss: 1.6415 - acc: 0.4850 - val_loss: 1.6893 - val_acc: 0.5102\n",
      "Epoch 50/100\n",
      "4163/4163 [==============================] - 1s 150us/step - loss: 1.6233 - acc: 0.5016 - val_loss: 1.6804 - val_acc: 0.5034\n",
      "Epoch 51/100\n",
      "4163/4163 [==============================] - 1s 151us/step - loss: 1.6181 - acc: 0.5013 - val_loss: 1.6761 - val_acc: 0.4952\n",
      "Epoch 52/100\n",
      "4163/4163 [==============================] - 1s 154us/step - loss: 1.6208 - acc: 0.4920 - val_loss: 1.8140 - val_acc: 0.4068\n",
      "Epoch 53/100\n",
      "4163/4163 [==============================] - 1s 148us/step - loss: 1.6670 - acc: 0.4614 - val_loss: 1.8409 - val_acc: 0.4653\n",
      "Epoch 54/100\n",
      "4163/4163 [==============================] - 1s 154us/step - loss: 1.6801 - acc: 0.4672 - val_loss: 1.6856 - val_acc: 0.4844\n",
      "Epoch 55/100\n",
      "4163/4163 [==============================] - 1s 152us/step - loss: 1.6266 - acc: 0.4946 - val_loss: 1.6832 - val_acc: 0.4871\n",
      "Epoch 56/100\n",
      "4163/4163 [==============================] - 1s 148us/step - loss: 1.6206 - acc: 0.4982 - val_loss: 1.6817 - val_acc: 0.4884\n",
      "Epoch 57/100\n",
      "4163/4163 [==============================] - 1s 157us/step - loss: 1.6274 - acc: 0.4946 - val_loss: 1.6771 - val_acc: 0.5061\n",
      "Epoch 58/100\n",
      "4163/4163 [==============================] - 1s 150us/step - loss: 1.6125 - acc: 0.4989 - val_loss: 1.6703 - val_acc: 0.5184\n",
      "Epoch 59/100\n",
      "4163/4163 [==============================] - 1s 163us/step - loss: 1.6832 - acc: 0.4583 - val_loss: 1.8068 - val_acc: 0.4585\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4163/4163 [==============================] - 1s 140us/step - loss: 1.7187 - acc: 0.4523 - val_loss: 1.7379 - val_acc: 0.4667\n",
      "Epoch 61/100\n",
      "4163/4163 [==============================] - 1s 145us/step - loss: 1.7130 - acc: 0.4735 - val_loss: 1.8902 - val_acc: 0.4571\n",
      "Epoch 62/100\n",
      "4163/4163 [==============================] - 1s 143us/step - loss: 1.6820 - acc: 0.4610 - val_loss: 1.9310 - val_acc: 0.4993\n",
      "Epoch 63/100\n",
      "4163/4163 [==============================] - 1s 143us/step - loss: 1.6310 - acc: 0.4972 - val_loss: 1.6744 - val_acc: 0.4912\n",
      "Epoch 64/100\n",
      "4163/4163 [==============================] - 1s 147us/step - loss: 1.6079 - acc: 0.4984 - val_loss: 1.6634 - val_acc: 0.5265\n",
      "Epoch 65/100\n",
      "4163/4163 [==============================] - 1s 148us/step - loss: 1.6084 - acc: 0.5085 - val_loss: 1.6686 - val_acc: 0.4871\n",
      "Epoch 66/100\n",
      "4163/4163 [==============================] - 1s 145us/step - loss: 1.6193 - acc: 0.4917 - val_loss: 1.6519 - val_acc: 0.5170\n",
      "Epoch 67/100\n",
      "4163/4163 [==============================] - 1s 141us/step - loss: 1.6018 - acc: 0.5090 - val_loss: 1.6637 - val_acc: 0.5320\n",
      "Epoch 68/100\n",
      "4163/4163 [==============================] - 1s 145us/step - loss: 1.7395 - acc: 0.4595 - val_loss: 1.7884 - val_acc: 0.4585\n",
      "Epoch 69/100\n",
      "4163/4163 [==============================] - 1s 139us/step - loss: 1.7115 - acc: 0.4478 - val_loss: 1.7260 - val_acc: 0.4707\n",
      "Epoch 70/100\n",
      "4163/4163 [==============================] - 1s 152us/step - loss: 1.6804 - acc: 0.4626 - val_loss: 1.7024 - val_acc: 0.4925\n",
      "Epoch 71/100\n",
      "4163/4163 [==============================] - 1s 152us/step - loss: 1.6307 - acc: 0.4891 - val_loss: 1.6939 - val_acc: 0.4912\n",
      "Epoch 72/100\n",
      "4163/4163 [==============================] - 1s 157us/step - loss: 1.6266 - acc: 0.5054 - val_loss: 1.6741 - val_acc: 0.4966\n",
      "Epoch 73/100\n",
      "4163/4163 [==============================] - 1s 150us/step - loss: 1.6005 - acc: 0.5028 - val_loss: 1.6945 - val_acc: 0.4871\n",
      "Epoch 74/100\n",
      "4163/4163 [==============================] - 1s 145us/step - loss: 1.6116 - acc: 0.5066 - val_loss: 1.6984 - val_acc: 0.4898\n",
      "Epoch 75/100\n",
      "4163/4163 [==============================] - 1s 145us/step - loss: 1.6141 - acc: 0.4965 - val_loss: 1.7149 - val_acc: 0.4544\n",
      "Epoch 76/100\n",
      "4163/4163 [==============================] - 1s 144us/step - loss: 1.6067 - acc: 0.5047 - val_loss: 1.7226 - val_acc: 0.4680\n",
      "Epoch 77/100\n",
      "4163/4163 [==============================] - 1s 155us/step - loss: 1.6066 - acc: 0.4992 - val_loss: 1.6904 - val_acc: 0.5048\n",
      "Epoch 78/100\n",
      "4163/4163 [==============================] - 1s 180us/step - loss: 1.6101 - acc: 0.5011 - val_loss: 1.6544 - val_acc: 0.4966\n",
      "Epoch 79/100\n",
      "4163/4163 [==============================] - 1s 199us/step - loss: 1.6017 - acc: 0.4980 - val_loss: 1.6594 - val_acc: 0.5116\n",
      "Epoch 80/100\n",
      "4163/4163 [==============================] - 1s 203us/step - loss: 1.6008 - acc: 0.5030 - val_loss: 1.6501 - val_acc: 0.4966\n",
      "Epoch 81/100\n",
      "4163/4163 [==============================] - 1s 186us/step - loss: 1.5894 - acc: 0.5133 - val_loss: 1.6498 - val_acc: 0.5061\n",
      "Epoch 82/100\n",
      "4163/4163 [==============================] - 1s 134us/step - loss: 1.5945 - acc: 0.5049 - val_loss: 1.6483 - val_acc: 0.5034\n",
      "Epoch 83/100\n",
      "4163/4163 [==============================] - 1s 153us/step - loss: 1.5817 - acc: 0.5112 - val_loss: 1.6484 - val_acc: 0.5197\n",
      "Epoch 84/100\n",
      "4163/4163 [==============================] - 1s 156us/step - loss: 1.5861 - acc: 0.5078 - val_loss: 1.6520 - val_acc: 0.5061\n",
      "Epoch 85/100\n",
      "4163/4163 [==============================] - 1s 194us/step - loss: 1.5890 - acc: 0.5136 - val_loss: 1.6492 - val_acc: 0.5048\n",
      "Epoch 86/100\n",
      "4163/4163 [==============================] - 1s 159us/step - loss: 1.5925 - acc: 0.5035 - val_loss: 1.7033 - val_acc: 0.5061\n",
      "Epoch 87/100\n",
      "4163/4163 [==============================] - 1s 144us/step - loss: 1.5993 - acc: 0.5100 - val_loss: 1.7292 - val_acc: 0.5061\n",
      "Epoch 88/100\n",
      "4163/4163 [==============================] - 1s 139us/step - loss: 1.5914 - acc: 0.5030 - val_loss: 1.7517 - val_acc: 0.5048\n",
      "Epoch 89/100\n",
      "4163/4163 [==============================] - 1s 130us/step - loss: 1.6561 - acc: 0.4763 - val_loss: 1.6615 - val_acc: 0.5238\n",
      "Epoch 90/100\n",
      "4163/4163 [==============================] - 1s 129us/step - loss: 1.6170 - acc: 0.5071 - val_loss: 1.6686 - val_acc: 0.5020\n",
      "Epoch 91/100\n",
      "4163/4163 [==============================] - 1s 129us/step - loss: 1.5976 - acc: 0.5078 - val_loss: 1.6894 - val_acc: 0.4871\n",
      "Epoch 92/100\n",
      "4163/4163 [==============================] - 1s 139us/step - loss: 1.5975 - acc: 0.5114 - val_loss: 1.7265 - val_acc: 0.4952\n",
      "Epoch 93/100\n",
      "4163/4163 [==============================] - 1s 146us/step - loss: 1.5819 - acc: 0.5143 - val_loss: 1.6887 - val_acc: 0.5116\n",
      "Epoch 94/100\n",
      "4163/4163 [==============================] - 1s 141us/step - loss: 1.5866 - acc: 0.5117 - val_loss: 1.9878 - val_acc: 0.4694\n",
      "Epoch 95/100\n",
      "4163/4163 [==============================] - 1s 133us/step - loss: 1.7201 - acc: 0.4689 - val_loss: 1.7222 - val_acc: 0.4667\n",
      "Epoch 96/100\n",
      "4163/4163 [==============================] - 1s 132us/step - loss: 1.6340 - acc: 0.4843 - val_loss: 1.7216 - val_acc: 0.4748\n",
      "Epoch 97/100\n",
      "4163/4163 [==============================] - 1s 136us/step - loss: 1.6142 - acc: 0.4948 - val_loss: 1.6911 - val_acc: 0.4803\n",
      "Epoch 98/100\n",
      "4163/4163 [==============================] - 1s 135us/step - loss: 1.6055 - acc: 0.5054 - val_loss: 1.6780 - val_acc: 0.5020\n",
      "Epoch 99/100\n",
      "4163/4163 [==============================] - 1s 147us/step - loss: 1.6099 - acc: 0.5080 - val_loss: 1.6709 - val_acc: 0.5143\n",
      "Epoch 100/100\n",
      "4163/4163 [==============================] - 1s 129us/step - loss: 1.5899 - acc: 0.5117 - val_loss: 1.7268 - val_acc: 0.5075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1369c3278>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
